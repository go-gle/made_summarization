{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2ebdb5-6882-4600-b4c0-649027d3d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import vocab as torch_vocab\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431545b0-f72a-4a8f-9d61-e9911728cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: gazeta/default\n",
      "Found cached dataset gazeta (/home/goncharovglebig/.cache/huggingface/datasets/IlyaGusev___gazeta/default/2.0.0/c329f0fc1c22ab6e43e0045ee659d0d43c647492baa2a6ab3a5ea7dac98cd552)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95a735780034ce58c6bf9ca4c03522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_metric = Rouge()\n",
    "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c110cd8-818f-4643-af5e-3c0890fe8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PG_MODEL_PATH = './pointer_gazeta.pth'\n",
    "PG_VOCAB_PATH = './gazeta_voc.pth'\n",
    "EXTR_MODEL_PATH = './extractor.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef91ca6-c205-4c33-8d1b-430fdc343213",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top3 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa6cf09-c237-4aaf-aeb8-ed0fc8889e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda0c80fd9874ef1a58add8255f3c5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.23926067161957473,\n",
       "  'p': 0.20639995255196122,\n",
       "  'f': 0.21514407167065555},\n",
       " 'rouge-2': {'r': 0.08792294667867649,\n",
       "  'p': 0.0733570704747428,\n",
       "  'f': 0.07724965555132388},\n",
       " 'rouge-l': {'r': 0.21655468997676802,\n",
       "  'p': 0.18726447349087771,\n",
       "  'f': 0.19495473475845665}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def top3(article):\n",
    "    return '.'.join(sent_tokenize(article)[:3])\n",
    "\n",
    "preds = []\n",
    "for art in tqdm(dataset['test']['text']):\n",
    "    preds.append(top3(art))\n",
    "\n",
    "rouge_metric.get_scores(preds, dataset['test']['summary'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84faf48e-f0c4-4aa3-8182-04c7dd2692fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Point Gen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35c5934-de08-4db7-be61-f7863075bfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0f0b8f79b040c684be35c800b30c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.20913465575729584,\n",
       "  'p': 0.23785843721588698,\n",
       "  'f': 0.21628455229856844},\n",
       " 'rouge-2': {'r': 0.070436885435969,\n",
       "  'p': 0.07686597047928244,\n",
       "  'f': 0.07100721675287897},\n",
       " 'rouge-l': {'r': 0.1882746409242634,\n",
       "  'p': 0.21415780838697196,\n",
       "  'f': 0.1947106309076359}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predictors import PGenPredictor\n",
    "\n",
    "\n",
    "ponter_model = PGenPredictor(\n",
    "    model_path=PG_MODEL_PATH,\n",
    "    vocab_path=PG_VOCAB_PATH,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "preds = []\n",
    "abst_lower = []\n",
    "for i in tqdm(range(len(dataset['test']['text']))):\n",
    "    preds.append(ponter_model.predict_one_sample(test_df['text'][i]))\n",
    "    abst_lower.append(dataset['test']['summary'][i].lower())\n",
    "\n",
    "rouge_metric.get_scores(preds, abst_lower, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25accf3-464d-4494-9cf7-12e3ee06ca8d",
   "metadata": {},
   "source": [
    "## Extractor + PGen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b847819b-a963-4253-9f30-5416a2176fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09accfd989bd4bcda7931b37347008e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.20907098421096082,\n",
       "  'p': 0.2378527022928294,\n",
       "  'f': 0.2162402917341788},\n",
       " 'rouge-2': {'r': 0.07042001648760098,\n",
       "  'p': 0.07685955000554483,\n",
       "  'f': 0.07099325174761899},\n",
       " 'rouge-l': {'r': 0.18822458148921,\n",
       "  'p': 0.21416762503221332,\n",
       "  'f': 0.1946788392502319}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predictors import ExtractionPGenPredictor\n",
    "\n",
    "\n",
    "extr_model = ExtractionPGenPredictor(\n",
    "    ext_model_path=EXTR_MODEL_PATH,\n",
    "    pg_model_path=PG_MODEL_PATH,\n",
    "    pg_vocab_path=PG_VOCAB_PATH,\n",
    "    device=device,\n",
    "    threshold=0.01\n",
    ")\n",
    "\n",
    "preds = []\n",
    "abst_lower = []\n",
    "for i in tqdm(range(len(dataset['test']['text']))):\n",
    "    preds.append(extr_model.predict_one_sample(dataset['test']['text'][i]))\n",
    "    abst_lower.append(dataset['test']['summary'][i].lower())\n",
    "\n",
    "rouge_metric.get_scores(preds, abst_lower, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c2c9f-497c-45b5-99a1-15b0382d446c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MBart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5086e-34d3-4e39-a82e-7448796add31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cc2394f47d407db0732be53404b744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goncharovglebig/summ_venv/lib/python3.7/site-packages/transformers/generation/utils.py:1392: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts = batch\n",
    "    input_ids = tokenizer(\n",
    "        texts,\n",
    "        max_length=600,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].to(device)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "test_loader = DataLoader(dataset['test']['text'],\n",
    "                         batch_size=1,\n",
    "                         collate_fn=collate_batch)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        output_ids = model.generate(\n",
    "            input_ids=batch,\n",
    "            no_repeat_ngram_size=4\n",
    "        )\n",
    "        preds = preds + [tokenizer.decode(tok, skip_special_tokens=True)\n",
    "                         for tok in output_ids]\n",
    "\n",
    "rouge_metric.get_scores(preds, dataset['test']['summary'], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a132d09e-8397-4cd3-9438-2855e2128761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.241287149048779,\n",
       "  'p': 0.23241927564091414,\n",
       "  'f': 0.2308946759881896},\n",
       " 'rouge-2': {'r': 0.08984618344552867,\n",
       "  'p': 0.08505692164663911,\n",
       "  'f': 0.08500935192604948},\n",
       " 'rouge-l': {'r': 0.2193604865789741,\n",
       "  'p': 0.2114669931119583,\n",
       "  'f': 0.2100055745953222}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_metric.get_scores(preds, dataset['test']['summary'], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12536d-8c0c-4b9d-b9e7-59e99ddc3b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summ_venv",
   "language": "python",
   "name": "summ_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
