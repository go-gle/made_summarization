{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262c3c92-8524-4319-8ef5-94e7708bfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import vocab as torch_vocab\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0e464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pointer_gen.model import PGen\n",
    "from src.pointer_gen.utils import get_counters, SummDataset, PointerDataPoint, SOS, EOS, PAD, OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51564f38-ca94-4764-b23c-88004975d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: gazeta/default\n",
      "Found cached dataset gazeta (/home/goncharovglebig/.cache/huggingface/datasets/IlyaGusev___gazeta/default/2.0.0/c329f0fc1c22ab6e43e0045ee659d0d43c647492baa2a6ab3a5ea7dac98cd552)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa12e01932644a439e904f3af774d656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\")\n",
    "\n",
    "\n",
    "specials = [SOS, EOS, PAD, OOV]\n",
    "\n",
    "\n",
    "train_df = dataset['train']\n",
    "val_df = dataset['validation']\n",
    "test_df = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8a0622-60c1-4b77-b441-ac259d15ad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0efe93d3140cfbc5ab950cc7f53be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39807"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creatae vocab\n",
    "# src_counts = get_counters(train_df['text'], train_df['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe9868a-c00d-4803-a8a5-6a0fa706c6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = torch_vocab(src_counts, min_freq=70, specials=specials)\n",
    "# vocab.set_default_index(vocab[OOV])\n",
    "# vocab = vocab\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b5df2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(vocab, 'gazeta_voc_43.pth')\n",
    "vocab = torch.load('gazeta_voc_43.pth')\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088aea73-0302-4a2a-9f71-b241a2adc41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d1d27206643b7a1aa49387c4bbd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ffd79c74e8404284a724b0f20284b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2730eb171443959f27592bbdb96e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = SummDataset(train_df['text'], train_df['summary'])\n",
    "val_dataset = SummDataset(val_df['text'], val_df['summary'])\n",
    "test_dataset = SummDataset(test_df['text'], test_df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706de80d-5749-4146-a78c-caf11dae4fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "del train_df\n",
    "del val_df\n",
    "del test_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collects batch for model\n",
    "    Returns:\n",
    "        enc_input_padded - padded art_idxs tensor\n",
    "        enc_input_ext_padded - padded art_exq_idxs tensor\n",
    "        enc_padding_mask - pad mask for enc_input_padded\n",
    "        extra_zeros - zeros for extented vocab\n",
    "        dec_input_padded - padded abs_idxs tensor\n",
    "        target_padded - padded abs_idxs extra tensor\n",
    "        target_padding_mask - pad mask for target_padded\n",
    "        target_lens - lens of decoder input\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    enc_list, enc_ext_list, dec_inp_list, target_list, oovs_len_list, target_lens_list, oovs = [], [], [], [], [], [], []\n",
    "    batch_size = len(batch)\n",
    "    for article, abstract in batch:\n",
    "        data_point = PointerDataPoint(article, abstract, vocab)\n",
    "        \n",
    "        enc_input = torch.tensor(np.array(data_point.art_idxs))\n",
    "        enc_input_ext = torch.tensor(np.array(data_point.art_ext_idxs))\n",
    "        dec_input = torch.tensor(np.array(data_point.abs_idxs))\n",
    "        target = torch.tensor(np.array(data_point.abs_ext_idxs))\n",
    "        \n",
    "        oovs.append(data_point.art_oovs)\n",
    "        oovs_len_list.append(len(data_point.art_oovs))\n",
    "        enc_list.append(enc_input)\n",
    "        dec_inp_list.append(dec_input)\n",
    "        enc_ext_list.append(enc_input_ext)\n",
    "        target_list.append(target)\n",
    "        target_lens_list.append(len(target))\n",
    "        \n",
    "\n",
    "    \n",
    "    enc_input_padded = pad_sequence(enc_list, padding_value=vocab[PAD]).T\n",
    "    enc_input_ext_padded = pad_sequence(enc_ext_list, padding_value=vocab[PAD]).T\n",
    "    target_padded = pad_sequence(target_list, padding_value=vocab[PAD]).T\n",
    "    dec_inp_padded = pad_sequence(dec_inp_list, padding_value=vocab[PAD]).T\n",
    "    \n",
    "    enc_padding_mask = enc_input_padded.ne(vocab[PAD]).long()\n",
    "    target_padding_mask = target_padded.ne(vocab[PAD]).long()\n",
    "    \n",
    "    target_lens = torch.tensor(target_lens_list)\n",
    "    \n",
    "    max_oovs = max(oovs_len_list)\n",
    "    extra_zeros = None\n",
    "    if max_oovs > 0:\n",
    "        extra_zeros = torch.zeros((batch_size, max_oovs), requires_grad=True)\n",
    "\n",
    "    return (enc_input_padded.to(device), enc_input_ext_padded.to(device),\n",
    "            enc_padding_mask.to(device), extra_zeros.to(device),\n",
    "            dec_inp_padded.to(device), target_padded.to(device),\n",
    "            target_padding_mask.to(device), target_lens.to(device),\n",
    "            oovs\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70b1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f643133-1ff5-469e-a39f-06e2222f463a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b30f72df4749fb98559c80675b249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78492fa6bff248c2bb143ff83cdbc3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #0 train loss 6.209452417189527, val loss 5.728891501426697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee7eb34738d4feab9f9c0abae0ee014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #1 train loss 5.8731587773730345, val loss 5.497052632570266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588931899284f6eb2749a84db991e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #2 train loss 5.660578015551946, val loss 5.343588787714641\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648742b54e0b43eaa31a51032c437e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #3 train loss 5.5086054940789095, val loss 5.230483644008636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212b723cc8b340739ea0b2dda0042bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #4 train loss 5.388398936312696, val loss 5.142677486419678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4756793f77b4639bc3e46eda68fa2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #5 train loss 5.2871017945775955, val loss 5.070227198203405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0743555aca394744ae6aad89d030c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #6 train loss 5.199694815130181, val loss 5.0089155595643176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c561988c2b4e1baad9916a251ef141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #7 train loss 5.122664967729688, val loss 4.955806299448013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791b53b0bfe340089cbd407c6771efb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training part\n",
    "model = PGen(\n",
    "    vocab_size=len(vocab),\n",
    "    emb_dim=128, \n",
    "    hid_dim=256)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = list(model.parameters())\n",
    "optimizer = torch.optim.Adagrad(params, lr=0.15, initial_accumulator_value=0.1)\n",
    "\n",
    "\n",
    "epoch_num = 30\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "for ep in tqdm(range(epoch_num)):\n",
    "    for train_batch in tqdm(train_loader):\n",
    "        src, src_ext, src_mask, extra_zeros, dec_inp, trg, trg_mask, target_lens, _ = train_batch\n",
    "\n",
    "        encoder_outputs, encoder_feature, s_t = model.encoder(src)\n",
    "        step_losses = []\n",
    "        batch_size = src.shape[0]\n",
    "        max_dec_len = dec_inp.shape[1]\n",
    "        \n",
    "        \n",
    "        # For first input\n",
    "        c_t = torch.zeros((batch_size, 2 * s_t[0].shape[-1]), requires_grad=True).to(device)\n",
    "        coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "        y_t = dec_inp[:, 0]\n",
    "        \n",
    "        for di in range(1, max_dec_len):\n",
    "            final_dist, s_t,  c_t, attn_dist, p_gen, next_coverage = model.decoder(y_t,\n",
    "                                                                                   s_t,\n",
    "                                                                                   encoder_outputs,\n",
    "                                                                                   encoder_feature,\n",
    "                                                                                   src_mask,\n",
    "                                                                                   c_t,\n",
    "                                                                                   extra_zeros,\n",
    "                                                                                   src_ext,\n",
    "                                                                                   coverage,\n",
    "                                                                                  )\n",
    "            target = trg[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs)\n",
    "\n",
    "            step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "            step_loss = step_loss + 0.1 * step_coverage_loss\n",
    "            coverage = next_coverage\n",
    "\n",
    "            step_mask = trg_mask[:, di]\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "            \n",
    "            # Next token\n",
    "            y_t = dec_inp[:, di]  # Teacher forcing\n",
    "            \n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / target_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.detach().item()\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "    #Validation loop\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            src, src_ext, src_mask, extra_zeros, dec_inp, trg, trg_mask, target_lens, _ = val_batch\n",
    "            encoder_outputs, encoder_feature, s_t = model.encoder(src)\n",
    "            step_losses = []\n",
    "            batch_size = src.shape[0]\n",
    "            max_dec_len = dec_inp.shape[1]\n",
    "            \n",
    "            # For first input\n",
    "            c_t = torch.zeros((batch_size, 2 * s_t[0].shape[-1]), requires_grad=True).to(device)\n",
    "            coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "            y_t = dec_inp[:, 0]\n",
    "            for di in range(1, max_dec_len):\n",
    "                final_dist, s_t,  c_t, attn_dist, p_gen, next_coverage = model.decoder(y_t,\n",
    "                                                                                       s_t,\n",
    "                                                                                       encoder_outputs,\n",
    "                                                                                       encoder_feature,\n",
    "                                                                                       src_mask,\n",
    "                                                                                       c_t,\n",
    "                                                                                       extra_zeros,\n",
    "                                                                                       src_ext,\n",
    "                                                                                       coverage,\n",
    "                                                                                      )\n",
    "                target = trg[:, di]\n",
    "                gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "                step_loss = -torch.log(gold_probs)\n",
    "\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + 0.1 * step_coverage_loss\n",
    "                coverage = next_coverage\n",
    "\n",
    "                step_mask = trg_mask[:, di]\n",
    "                step_loss = step_loss * step_mask\n",
    "                step_losses.append(step_loss)\n",
    "                \n",
    "                y_t = dec_inp[:, di]  # Teacher forcing\n",
    "\n",
    "            sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "            batch_avg_loss = sum_losses / target_lens\n",
    "            loss = torch.mean(batch_avg_loss)\n",
    "            val_loss = loss.item()\n",
    "            val_loss_list.append(val_loss)\n",
    "            torch.save(model, f'pointer_gazeta_{ep}.pth')\n",
    "        \n",
    "        print(f'For epoch #{ep} train loss {np.mean(train_loss_list[-250_000:])}, val loss {np.mean(val_loss_list[-10_000:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72f8b-8e41-45eb-8300-cf61948a25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list, label='train');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1a65d-7943-471d-b483-7f111eea05eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(val_loss_list, label='val');\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summ_venv",
   "language": "python",
   "name": "summ_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}