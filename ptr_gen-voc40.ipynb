{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262c3c92-8524-4319-8ef5-94e7708bfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import vocab as torch_vocab\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from random import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0e464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PGen\n",
    "from utils import (get_counters, encode, encode_ext_abstract, encode_ext_article,\n",
    "                   decode, preporocess_text, SummDataset, PointerDataPoint,\n",
    "                  SOS, EOS, PAD, OOV)\n",
    "\n",
    "from decode import BeamSearchNode, beam_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51564f38-ca94-4764-b23c-88004975d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: gazeta/default\n",
      "Found cached dataset gazeta (/home/goncharovglebig/.cache/huggingface/datasets/IlyaGusev___gazeta/default/2.0.0/c329f0fc1c22ab6e43e0045ee659d0d43c647492baa2a6ab3a5ea7dac98cd552)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa12e01932644a439e904f3af774d656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\")\n",
    "\n",
    "\n",
    "specials = [SOS, EOS, PAD, OOV]\n",
    "\n",
    "\n",
    "train_df = dataset['train']\n",
    "val_df = dataset['validation']\n",
    "test_df = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8a0622-60c1-4b77-b441-ac259d15ad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0efe93d3140cfbc5ab950cc7f53be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39807"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creatae vocab\n",
    "# src_counts = get_counters(train_df['text'], train_df['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe9868a-c00d-4803-a8a5-6a0fa706c6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = torch_vocab(src_counts, min_freq=70, specials=specials)\n",
    "# vocab.set_default_index(vocab[OOV])\n",
    "# vocab = vocab\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b5df2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(vocab, 'gazeta_voc_43.pth')\n",
    "vocab = torch.load('gazeta_voc_43.pth')\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088aea73-0302-4a2a-9f71-b241a2adc41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d1d27206643b7a1aa49387c4bbd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ffd79c74e8404284a724b0f20284b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2730eb171443959f27592bbdb96e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6793 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = SummDataset(train_df['text'], train_df['summary'])\n",
    "val_dataset = SummDataset(val_df['text'], val_df['summary'])\n",
    "test_dataset = SummDataset(test_df['text'], test_df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706de80d-5749-4146-a78c-caf11dae4fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "del train_df\n",
    "del val_df\n",
    "del test_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collects batch for model\n",
    "    Returns:\n",
    "        enc_input_padded - padded art_idxs tensor\n",
    "        enc_input_ext_padded - padded art_exq_idxs tensor\n",
    "        enc_padding_mask - pad mask for enc_input_padded\n",
    "        extra_zeros - zeros for extented vocab\n",
    "        dec_input_padded - padded abs_idxs tensor\n",
    "        target_padded - padded abs_idxs extra tensor\n",
    "        target_padding_mask - pad mask for target_padded\n",
    "        target_lens - lens of decoder input\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    enc_list, enc_ext_list, dec_inp_list, target_list, oovs_len_list, target_lens_list, oovs = [], [], [], [], [], [], []\n",
    "    batch_size = len(batch)\n",
    "    for article, abstract in batch:\n",
    "        data_point = PointerDataPoint(article, abstract, vocab)\n",
    "        \n",
    "        enc_input = torch.tensor(np.array(data_point.art_idxs))\n",
    "        enc_input_ext = torch.tensor(np.array(data_point.art_ext_idxs))\n",
    "        dec_input = torch.tensor(np.array(data_point.abs_idxs))\n",
    "        target = torch.tensor(np.array(data_point.abs_ext_idxs))\n",
    "        \n",
    "        oovs.append(data_point.art_oovs)\n",
    "        oovs_len_list.append(len(data_point.art_oovs))\n",
    "        enc_list.append(enc_input)\n",
    "        dec_inp_list.append(dec_input)\n",
    "        enc_ext_list.append(enc_input_ext)\n",
    "        target_list.append(target)\n",
    "        target_lens_list.append(len(target))\n",
    "        \n",
    "\n",
    "    \n",
    "    enc_input_padded = pad_sequence(enc_list, padding_value=vocab[PAD]).T\n",
    "    enc_input_ext_padded = pad_sequence(enc_ext_list, padding_value=vocab[PAD]).T\n",
    "    target_padded = pad_sequence(target_list, padding_value=vocab[PAD]).T\n",
    "    dec_inp_padded = pad_sequence(dec_inp_list, padding_value=vocab[PAD]).T\n",
    "    \n",
    "    enc_padding_mask = enc_input_padded.ne(vocab[PAD]).long()\n",
    "    target_padding_mask = target_padded.ne(vocab[PAD]).long()\n",
    "    \n",
    "    target_lens = torch.tensor(target_lens_list)\n",
    "    \n",
    "    max_oovs = max(oovs_len_list)\n",
    "    extra_zeros = None\n",
    "    if max_oovs > 0:\n",
    "        extra_zeros = torch.zeros((batch_size, max_oovs), requires_grad=True)\n",
    "\n",
    "    return (enc_input_padded.to(device), enc_input_ext_padded.to(device),\n",
    "            enc_padding_mask.to(device), extra_zeros.to(device),\n",
    "            dec_inp_padded.to(device), target_padded.to(device),\n",
    "            target_padding_mask.to(device), target_lens.to(device),\n",
    "            oovs\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70b1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=32,\n",
    "                          collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f643133-1ff5-469e-a39f-06e2222f463a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b30f72df4749fb98559c80675b249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78492fa6bff248c2bb143ff83cdbc3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #0 train loss 6.209452417189527, val loss 5.728891501426697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee7eb34738d4feab9f9c0abae0ee014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #1 train loss 5.8731587773730345, val loss 5.497052632570266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588931899284f6eb2749a84db991e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #2 train loss 5.660578015551946, val loss 5.343588787714641\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648742b54e0b43eaa31a51032c437e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #3 train loss 5.5086054940789095, val loss 5.230483644008636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212b723cc8b340739ea0b2dda0042bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #4 train loss 5.388398936312696, val loss 5.142677486419678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4756793f77b4639bc3e46eda68fa2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #5 train loss 5.2871017945775955, val loss 5.070227198203405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0743555aca394744ae6aad89d030c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #6 train loss 5.199694815130181, val loss 5.0089155595643176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c561988c2b4e1baad9916a251ef141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch #7 train loss 5.122664967729688, val loss 4.955806299448013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791b53b0bfe340089cbd407c6771efb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training part\n",
    "model = PGen(\n",
    "    vocab_size=len(vocab),\n",
    "    emb_dim=128, \n",
    "    hid_dim=256)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = list(model.parameters())\n",
    "optimizer = torch.optim.Adagrad(params, lr=0.15, initial_accumulator_value=0.1)\n",
    "\n",
    "\n",
    "epoch_num = 30\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "for ep in tqdm(range(epoch_num)):\n",
    "    for train_batch in tqdm(train_loader):\n",
    "        src, src_ext, src_mask, extra_zeros, dec_inp, trg, trg_mask, target_lens, _ = train_batch\n",
    "\n",
    "        encoder_outputs, encoder_feature, s_t_1 = model.encoder(src)\n",
    "        step_losses = []\n",
    "        batch_size = src.shape[0]\n",
    "        max_dec_len = dec_inp.shape[1]\n",
    "        \n",
    "        \n",
    "        # For first input\n",
    "        c_t_1 = torch.zeros((batch_size, 2 * s_t_1[0].shape[-1]), requires_grad=True).to(device)\n",
    "        coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "        y_t_1 = dec_inp[:, 0]\n",
    "        \n",
    "        for di in range(1, max_dec_len):\n",
    "            final_dist, s_t_1,  c_t_1, attn_dist, p_gen, next_coverage = model.decoder(y_t_1, s_t_1,\n",
    "                                                                                       encoder_outputs,\n",
    "                                                                                       encoder_feature,\n",
    "                                                                                       src_mask,\n",
    "                                                                                       c_t_1,\n",
    "                                                                                       extra_zeros,\n",
    "                                                                                       src_ext,\n",
    "                                                                                       coverage,\n",
    "                                                                                      )\n",
    "            target = trg[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs)\n",
    "\n",
    "            step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "            step_loss = step_loss + 0.1 * step_coverage_loss\n",
    "            coverage = next_coverage\n",
    "\n",
    "            step_mask = trg_mask[:, di]\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "            \n",
    "            # Next token\n",
    "            y_t_1 = dec_inp[:, di]  # Teacher forcing\n",
    "            \n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / target_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.detach().item()\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "    #Validation loop\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            src, src_ext, src_mask, extra_zeros, dec_inp, trg, trg_mask, target_lens, _ = val_batch\n",
    "            encoder_outputs, encoder_feature, s_t_1 = model.encoder(src)\n",
    "            step_losses = []\n",
    "            batch_size = src.shape[0]\n",
    "            max_dec_len = dec_inp.shape[1]\n",
    "            \n",
    "            # For first input\n",
    "            c_t_1 = torch.zeros((batch_size, 2 * s_t_1[0].shape[-1]), requires_grad=True).to(device)\n",
    "            coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "            y_t_1 = dec_inp[:, 0]\n",
    "            for di in range(1, max_dec_len):\n",
    "                final_dist, s_t_1,  c_t_1, attn_dist, p_gen, next_coverage = model.decoder(y_t_1, s_t_1,\n",
    "                                                                                           encoder_outputs,\n",
    "                                                                                           encoder_feature,\n",
    "                                                                                           src_mask,\n",
    "                                                                                           c_t_1,\n",
    "                                                                                           extra_zeros,\n",
    "                                                                                           src_ext,\n",
    "                                                                                           coverage,\n",
    "                                                                                          )\n",
    "                target = trg[:, di]\n",
    "                gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "                step_loss = -torch.log(gold_probs)\n",
    "\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + 0.1 * step_coverage_loss\n",
    "                coverage = next_coverage\n",
    "\n",
    "                step_mask = trg_mask[:, di]\n",
    "                step_loss = step_loss * step_mask\n",
    "                step_losses.append(step_loss)\n",
    "                \n",
    "                y_t_1 = dec_inp[:, di]  # Teacher forcing\n",
    "\n",
    "            sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "            batch_avg_loss = sum_losses / target_lens\n",
    "            loss = torch.mean(batch_avg_loss)\n",
    "            val_loss = loss.item()\n",
    "            val_loss_list.append(val_loss)\n",
    "            torch.save(model, f'pointer_gazeta_{ep}.pth')\n",
    "        \n",
    "        print(f'For epoch #{ep} train loss {np.mean(train_loss_list[-250_000:])}, val loss {np.mean(val_loss_list[-10_000:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72f8b-8e41-45eb-8300-cf61948a25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list, label='train');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1a65d-7943-471d-b483-7f111eea05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_list, label='val');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = torch.load('ponter_gazeta_0').to(device)\n",
    "point = PointerDataPoint(val_dataset[3][0],\n",
    "                         val_dataset[3][1],\n",
    "                         vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d798ca-eee4-487c-bfb3-3b01077b4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src = torch.tensor(np.array(point.art_idxs)).unsqueeze(0).to(device)\n",
    "src_mask = torch.ones(len(point.art_idxs)).unsqueeze(0).to(device)\n",
    "src_ext = torch.tensor(np.array(point.art_ext_idxs)).unsqueeze(0).to(device)\n",
    "extra_zeros = torch.zeros(len(point.art_oovs)).unsqueeze(0).to(device)\n",
    "\n",
    "model.encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea2b27-803d-475a-8041-a1fe8c1c9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, encoder_feature, s_t_1 = model.encoder(src)\n",
    "    batch_size = src.shape[0]\n",
    "    max_dec_len = 100\n",
    "\n",
    "    # For first input\n",
    "    c_t_1 = torch.zeros((batch_size, 2 * s_t_1[0].shape[-1]), requires_grad=True).to(device)\n",
    "    coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "    y_t_1 = src[:, 0]\n",
    "    for di in range(1, 100):\n",
    "        final_dist, s_t_1,  c_t_1, attn_dist, p_gen, next_coverage = model.decoder(y_t_1, s_t_1,\n",
    "                                                                                   encoder_outputs,\n",
    "                                                                                   encoder_feature,\n",
    "                                                                                   src_mask,\n",
    "                                                                                   c_t_1,\n",
    "                                                                                   extra_zeros,\n",
    "                                                                                   src_ext,\n",
    "                                                                                   coverage,\n",
    "                                                                                  )\n",
    "\n",
    "        coverage = next_coverage\n",
    "        y_t_1 = final_dist.argmax(1)\n",
    "        preds.append(y_t_1.item())\n",
    "        y_t_1 = y_t_1 if y_t_1 < len(vocab) else torch.LongTensor([vocab[OOV]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f532a-2a6e-43f1-b9b9-1df27b05433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(decode(point.abs_ext_idxs, vocab, point.art_oovs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a4e07-5b7a-47a5-b7b2-fe477cb1e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' ' .join(decode(preds, vocab, point.art_oovs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6f7b6-85b9-49ed-82cc-e92af1474435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict in a greedy way\n",
    "targets_list, predicts_list, oovs_list = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader):\n",
    "            src, src_ext, src_mask, extra_zeros, dec_inp, trg, trg_mask, target_lens, oov = val_batch\n",
    "            encoder_outputs, encoder_feature, s_t_1 = model.encoder(src)\n",
    "            step_losses = []\n",
    "            batch_size = src.shape[0]\n",
    "            max_dec_len = dec_inp.shape[1]\n",
    "            predicts = np.zeros((batch_size, max_dec_len), dtype='int')\n",
    "            \n",
    "            # For first input\n",
    "            c_t_1 = torch.zeros((batch_size, 2 * s_t_1[0].shape[-1]), requires_grad=True).to(device)\n",
    "            coverage = torch.zeros((src.shape), requires_grad=True).to(device)\n",
    "            y_t_1 = dec_inp[:, 0]\n",
    "            for di in range(1, max_dec_len):\n",
    "                final_dist, s_t_1,  c_t_1, attn_dist, p_gen, next_coverage = model.decoder(y_t_1, s_t_1,\n",
    "                                                                                           encoder_outputs,\n",
    "                                                                                           encoder_feature,\n",
    "                                                                                           src_mask,\n",
    "                                                                                           c_t_1,\n",
    "                                                                                           extra_zeros,\n",
    "                                                                                           src_ext,\n",
    "                                                                                           coverage,\n",
    "                                                                                          )\n",
    "\n",
    "                y_t_1 = final_dist.argmax(1)\n",
    "                predicts[:, di] = y_t_1.cpu().numpy()\n",
    "                y_t_1[y_t_1 >= len(vocab)] = vocab[OOV]\n",
    "            \n",
    "            targets_list = targets_list + trg.cpu().numpy().tolist()\n",
    "            predicts_list = predicts_list + predicts.tolist()\n",
    "            oovs_list = oovs_list + oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871169d0-d798-47e8-a58f-29a7f8b38e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "print(f\"= {' '.join(decode(targets_list[ind], vocab, oovs_list[ind]))}\")\n",
    "print(f\"> {' '.join(decode(predicts_list[ind], vocab, oovs_list[ind]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206ac55-3198-405b-88b9-960f11e771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_targets = []\n",
    "decoded_predicts = []\n",
    "\n",
    "for i in tqdm(range(len(predicts_list))):\n",
    "    decoded_targets.append(' '.join(decode(targets_list[i], vocab, oovs_list[i])).replace('<SOS>', '').replace('<PAD>', '').replace('<EOS>', ''))\n",
    "    decoded_predicts.append(' '.join(decode(predicts_list[i], vocab, oovs_list[i])).replace('<SOS>', '').replace('<PAD>', '').replace('<EOS>', ''))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec0526-55b6-4467-be1c-1f03308a4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "rouge.get_scores(decoded_predicts, decoded_targets, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6251891-0cde-444c-adec-83083cc2141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### top3 baseline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def top3_baseline(text):\n",
    "    return ' '.join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab424984-b695-4942-b128-0560ab863817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541b3444fa6c491295e7400b458aadd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_targets = []\n",
    "decoded_predicts = []\n",
    "\n",
    "for data in tqdm(val_dataset):\n",
    "    art, abst = data\n",
    "    decoded_targets.append(' '.join(abst))\n",
    "    decoded_predicts.append(top3_baseline(' '.join(art)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cfa1fb5-7f0b-453b-a703-28fe74093864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.3397863654959966,\n",
       "  'p': 0.2918713845352074,\n",
       "  'f': 0.30560469773278953},\n",
       " 'rouge-2': {'r': 0.15051781779285675,\n",
       "  'p': 0.12250571423980426,\n",
       "  'f': 0.13050782714767825},\n",
       " 'rouge-l': {'r': 0.30222040078967655,\n",
       "  'p': 0.2599901595337212,\n",
       "  'f': 0.27202348650584385}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(decoded_predicts, decoded_targets, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6397e9dd-2f17-4723-94c3-acd31608780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c499ade7a9f84855908048054e1ea4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 's_t_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1339/76956381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mSOS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSOS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mEOS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                )\n",
      "\u001b[0;32m~/project/ptr_gen/v2/decode.py\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(model, max_len, beam_width, src, src_mask, src_ext, extra_zeros, device, SOS, EOS, vocab)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mterm_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms_t_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mcov_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_t_1' is not defined"
     ]
    }
   ],
   "source": [
    "src = torch.tensor(np.array(point.art_idxs)).unsqueeze(0).to(device)\n",
    "src_mask = torch.ones(len(point.art_idxs)).unsqueeze(0).to(device)\n",
    "src_ext = torch.tensor(np.array(point.art_ext_idxs)).unsqueeze(0).to(device)\n",
    "extra_zeros = torch.zeros(len(point.art_oovs)).unsqueeze(0).to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded = beam_decode(model=model,\n",
    "                max_len=100,\n",
    "                beam_width=5,\n",
    "                src=src,\n",
    "                src_mask=src_mask,\n",
    "                src_ext=src_ext,\n",
    "                extra_zeros=extra_zeros,\n",
    "                device=device,\n",
    "                SOS=SOS,\n",
    "                EOS=EOS,\n",
    "                vocab=vocab,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ecc65-9b01-4c39-966b-ecdc70abfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' ' .join(decode([i.item() for i in decoded], vocab, point.art_oovs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7a995-b3d7-4da2-8b86-129cbc0876b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summ_venv",
   "language": "python",
   "name": "summ_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
